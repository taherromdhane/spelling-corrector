{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bX2wzx3i5M3Z"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure these libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q4UiQXIxMMN1",
    "outputId": "1bcc340d-832e-4b9e-e707-ef36edc7af92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\user\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (1.1.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install unidecode\n",
    "#!pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.5.1-py3-none-any.whl\n",
    "#!pip3 install numpy\n",
    "#!pip3 install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SljrfenJ5Kjc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-27d09ce73c31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import unidecode\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras import optimizers, metrics, backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSOpDTWceany"
   },
   "outputs": [],
   "source": [
    "SOS = '\\t' # start of sequence.\n",
    "EOS = '*' # end of sequence.\n",
    "CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YK6qZZW85evb"
   },
   "source": [
    "Utility function to calculate accuracy and loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1VrAbJ75C_i"
   },
   "outputs": [],
   "source": [
    "def truncated_acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
    "    return K.mean(acc, axis=-1)\n",
    "\n",
    "\n",
    "def truncated_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    loss = K.categorical_crossentropy(\n",
    "        target=y_true, output=y_pred, from_logits=False)\n",
    "    return K.mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UozP-OH15Y5N"
   },
   "source": [
    "Utility Class to encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XU2ygRp5LPj"
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "          chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.size = len(self.chars)\n",
    "    \n",
    "    def encode(self, C, nb_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "          C: string, to be encoded.\n",
    "          nb_rows: Number of rows in the returned one-hot encoding. This is\n",
    "          used to keep the # of rows for each data the same via padding.\n",
    "        \"\"\"\n",
    "        x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char2index[c]] = 1.0\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "          x: A vector or 2D array of probabilities or one-hot encodings,\n",
    "          or a vector of character indices (used with `calc_argmax=False`).\n",
    "          calc_argmax: Whether to find the character index with maximum\n",
    "          probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            indices = x.argmax(axis=-1)\n",
    "        else:\n",
    "            indices = x\n",
    "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
    "        return indices, chars\n",
    "\n",
    "    def sample_multinomial(self, preds, temperature=1.0):\n",
    "        \"\"\"Sample index and character output from `preds`,\n",
    "        an array of softmax probabilities with shape (1, 1, nb_chars).\n",
    "        \"\"\"\n",
    "        # Reshaped to 1D array of shape (nb_chars,).\n",
    "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probs = np.random.multinomial(1, preds, 1)\n",
    "        index = np.argmax(probs)\n",
    "        char  = self.index2char[index]\n",
    "        return index, char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PKlJAW35uSx"
   },
   "source": [
    "Utility function to read the text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kYyE9On5s58"
   },
   "outputs": [],
   "source": [
    "def read_text(data_path, list_of_books):\n",
    "    text = ''\n",
    "    for book in list_of_books:\n",
    "        file_path = os.path.join(data_path, book)\n",
    "        strings = unidecode.unidecode(open(file_path).read())\n",
    "        text += strings + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edBiuojC768g"
   },
   "source": [
    "Utility function to split text into tokens and remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb_i3PSc5tha"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
    "              for token in re.split(\"[-\\n ]\", text)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdz9LNKi8Tci"
   },
   "source": [
    "Utility function that adds errors to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlDuRLe98RIr"
   },
   "outputs": [],
   "source": [
    "def add_speling_erors(token, error_rate):\n",
    "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
    "    assert(0.0 <= error_rate < 1.0)\n",
    "    if len(token) < 3:\n",
    "        return token\n",
    "    rand = np.random.rand()\n",
    "    # Here are 4 different ways spelling mistakes can occur,\n",
    "    # each of which has equal chance.\n",
    "    prob = error_rate / 4.0\n",
    "    if rand < prob:\n",
    "        # Replace a character with a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index + 1:]\n",
    "    elif prob < rand < prob * 2:\n",
    "        # Delete a character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
    "    elif prob * 2 < rand < prob * 3:\n",
    "        # Add a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index:]\n",
    "    elif prob * 3 < rand < prob * 4:\n",
    "        # Transpose 2 characters.\n",
    "        random_char_index = np.random.randint(len(token) - 1)\n",
    "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
    "                + token[random_char_index] + token[random_char_index + 2:]\n",
    "    else:\n",
    "        # No spelling errors.\n",
    "        pass\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-N53jzO8lex"
   },
   "source": [
    "Utility function to transform tokens into model inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mTUSY8v8XNz"
   },
   "outputs": [],
   "source": [
    "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
    "    \"\"\"Transform tokens into model inputs and targets.\n",
    "    All inputs and targets are padded to maxlen with EOS character.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        print('Shuffling data.')\n",
    "        np.random.shuffle(tokens)\n",
    "    encoder_tokens = []\n",
    "    decoder_tokens = []\n",
    "    target_tokens = []\n",
    "    for token in tokens:\n",
    "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
    "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
    "        encoder_tokens.append(encoder)\n",
    "    \n",
    "        decoder = SOS + token\n",
    "        decoder += EOS * (maxlen - len(decoder))\n",
    "        decoder_tokens.append(decoder)\n",
    "    \n",
    "        target = decoder[1:]\n",
    "        target += EOS * (maxlen - len(target))\n",
    "        target_tokens.append(target)\n",
    "        \n",
    "        assert(len(encoder) == len(decoder) == len(target))\n",
    "    return encoder_tokens, decoder_tokens, target_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QuySI2eu8zmk"
   },
   "source": [
    "# Utility function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBVwl8q98wAx"
   },
   "outputs": [],
   "source": [
    "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
    "    \"\"\"Split data into chunks of `batch_size` examples.\"\"\"\n",
    "    def generate(tokens, reverse):\n",
    "        while(True): # This flag yields an infinite generator.\n",
    "            for token in tokens:\n",
    "                if reverse:\n",
    "                    token = token[::-1]\n",
    "                yield token\n",
    "    \n",
    "    token_iterator = generate(tokens, reverse)\n",
    "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
    "                          dtype=np.float32)\n",
    "    while(True):\n",
    "        for i in range(batch_size):\n",
    "            token = next(token_iterator)\n",
    "            data_batch[i] = ctable.encode(token, maxlen)\n",
    "        yield data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V54TCiNW82Qq"
   },
   "outputs": [],
   "source": [
    "def datagen(encoder_iter, decoder_iter, target_iter):\n",
    "    \"\"\"Utility function to load data into required model format.\"\"\"\n",
    "    inputs = zip(encoder_iter, decoder_iter)\n",
    "    while(True):\n",
    "        encoder_input, decoder_input = next(inputs)\n",
    "        target = next(target_iter)\n",
    "        yield ([encoder_input, decoder_input], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOf6DjNB87A6"
   },
   "outputs": [],
   "source": [
    "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
    "                     maxlen, reverse, encoder_model, decoder_model,\n",
    "                     nb_examples, sample_mode='argmax', random=True):\n",
    "    input_tokens = []\n",
    "    target_tokens = []\n",
    "    \n",
    "    if random:\n",
    "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
    "    else:\n",
    "        indices = range(nb_examples)\n",
    "        \n",
    "    for index in indices:\n",
    "        input_tokens.append(inputs[index])\n",
    "        target_tokens.append(targets[index])\n",
    "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
    "                            nb_examples, reverse)\n",
    "    input_sequences = next(input_sequences)\n",
    "    \n",
    "    # Procedure for inference mode (sampling):\n",
    "    # 1) Encode input and retrieve initial decoder state.\n",
    "    # 2) Run one step of decoder with this initial state\n",
    "    #    and a start-of-sequence character as target.\n",
    "    #    Output will be the next target character.\n",
    "    # 3) Repeat with the current target character and current states.\n",
    "\n",
    "    # Encode the input as state vectors.    \n",
    "    states_value = encoder_model.predict(input_sequences)\n",
    "    \n",
    "    # Create batch of empty target sequences of length 1 character.\n",
    "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "    # Populate the first element of target sequence\n",
    "    # with the start-of-sequence character.\n",
    "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences.\n",
    "    # Exit condition: either hit max character limit\n",
    "    # or encounter end-of-sequence character.\n",
    "    decoded_tokens = [''] * nb_examples\n",
    "    for _ in range(maxlen):\n",
    "        # `char_probs` has shape\n",
    "        # (nb_examples, 1, nb_target_chars)\n",
    "        char_probs, h, c = decoder_model.predict(\n",
    "            [target_sequences] + states_value)\n",
    "\n",
    "        # Reset the target sequences.\n",
    "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "\n",
    "        # Sample next character using argmax or multinomial mode.\n",
    "        sampled_chars = []\n",
    "        for i in range(nb_examples):\n",
    "            if sample_mode == 'argmax':\n",
    "                next_index, next_char = target_ctable.decode(\n",
    "                    char_probs[i], calc_argmax=True)\n",
    "            elif sample_mode == 'multinomial':\n",
    "                next_index, next_char = target_ctable.sample_multinomial(\n",
    "                    char_probs[i], temperature=0.5)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
    "            decoded_tokens[i] += next_char\n",
    "            sampled_chars.append(next_char) \n",
    "            # Update target sequence with index of next character.\n",
    "            target_sequences[i, 0, next_index] = 1.0\n",
    "\n",
    "        stop_char = set(sampled_chars)\n",
    "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
    "            break\n",
    "            \n",
    "        # Update states.\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    # Sampling finished.\n",
    "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in input_tokens]\n",
    "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in target_tokens]\n",
    "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in decoded_tokens]\n",
    "    return input_tokens, target_tokens, decoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcJSBVLX87kU"
   },
   "outputs": [],
   "source": [
    "def restore_model(path_to_full_model, hidden_size):\n",
    "    \"\"\"Restore model to construct the encoder and decoder.\"\"\"\n",
    "    model = load_model(path_to_full_model, custom_objects={\n",
    "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
    "    \n",
    "    encoder_inputs = model.input[0] # encoder_data\n",
    "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
    "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
    "    \n",
    "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
    "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1] # decoder_data\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.get_layer('decoder_lstm')\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_softmax = model.get_layer('decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb3PjlMU-GHk"
   },
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KqEjnwwXAl1K"
   },
   "source": [
    "importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xg4PXN3B9LrQ"
   },
   "outputs": [],
   "source": [
    "VAL_MAXLEN = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDC2-P2UApGZ"
   },
   "outputs": [],
   "source": [
    "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
    "    \"\"\"Adapted from:\n",
    "    https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the main model consisting of encoder and decoder.\n",
    "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
    "                           name='encoder_data')\n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=True, return_state=False,\n",
    "                        name='encoder_lstm_1')\n",
    "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
    "    \n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=False, return_state=True,\n",
    "                        name='encoder_lstm_2')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
    "                           name='decoder_data')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the return\n",
    "    # states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
    "                        return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
    "                            name='decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "\n",
    "    # The main model will turn `encoder_input_data` & `decoder_input_data`\n",
    "    # into `decoder_target_data`\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                  outputs=decoder_outputs)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
    "    \n",
    "    # Define the encoder model separately.\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    # Define the decoder model separately.\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lh6yieAnBVPi"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0m0plNC-BY__"
   },
   "source": [
    "First of all we define some variables we'll need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOAwiQz-Ay8j"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f2a5bebce8f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0merror_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "error_rate = 0.8\n",
    "hidden_size = 512\n",
    "nb_epochs = 100\n",
    "train_batch_size = 128\n",
    "val_batch_size = 256\n",
    "sample_mode = 'argmax'\n",
    "reverse = True\n",
    "\n",
    "data_path = './data'\n",
    "#data_path = './'\n",
    "\n",
    "train_books = ['jfleg dev.txt']\n",
    "val_books = ['jfleg dev.txt']\n",
    "\n",
    "train_books = ['nietzsche.txt', 'pride_and_prejudice.txt', 'shakespeare.txt', 'war_and_peace.txt', 'wonderland.txt']\n",
    "val_books = ['nietzsche.txt', 'pride_and_prejudice.txt', 'shakespeare.txt', 'war_and_peace.txt', 'wonderland.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z0HfehiBjQb"
   },
   "source": [
    "Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "tEJmOiY0Bisc",
    "outputId": "20a44973-b49e-4985-c912-092aea3147cd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-969cccf38da7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_books\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# `maxlen` is the length of the longest word in the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "text  = read_text(data_path, train_books)\n",
    "vocab = tokenize(text)\n",
    "vocab = list(filter(None, set(vocab)))\n",
    "\n",
    "# `maxlen` is the length of the longest word in the vocabulary\n",
    "# plus two SOS and EOS characters.\n",
    "maxlen = max([len(token) for token in vocab]) + 2\n",
    "train_encoder, train_decoder, train_target = transform(\n",
    "vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
    "print(train_encoder[:10])\n",
    "print(train_decoder[:10])\n",
    "print(train_target[:10])\n",
    "\n",
    "input_chars = set(' '.join(train_encoder))\n",
    "target_chars = set(' '.join(train_decoder))\n",
    "nb_input_chars = len(input_chars)\n",
    "nb_target_chars = len(target_chars)\n",
    "\n",
    "print('Size of training vocabulary =', len(vocab))\n",
    "print('Number of unique input characters:', nb_input_chars)\n",
    "print('Number of unique target characters:', nb_target_chars)\n",
    "print('Max sequence length in the training set:', maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEUiXgUGBmqz"
   },
   "source": [
    "Prepare validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "E5adO-_PBsOt",
    "outputId": "fa5189f2-a23e-4d2f-af49-f51311c714a0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-25e4a4b50b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_books\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_maxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "text = read_text(data_path, val_books)\n",
    "val_tokens = tokenize(text)\n",
    "val_tokens = list(filter(None, val_tokens))\n",
    "\n",
    "val_maxlen = max([len(token) for token in val_tokens]) + 2\n",
    "val_encoder, val_decoder, val_target = transform(\n",
    "val_tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
    "print(val_encoder[:10])\n",
    "print(val_decoder[:10])\n",
    "print(val_target[:10])\n",
    "print('Number of non-unique validation tokens =', len(val_tokens))\n",
    "print('Max sequence length in the validation set:', val_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNzmW1PWBw51"
   },
   "source": [
    "Define training and evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "19rA7KEHBvht",
    "outputId": "f8fcd391-d810-4c25-f9b9-c68d9fa49bd8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cfd6c65035b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_ctable\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mCharacterTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtarget_ctable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCharacterTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_chars' is not defined"
     ]
    }
   ],
   "source": [
    "input_ctable  = CharacterTable(input_chars)\n",
    "target_ctable = CharacterTable(target_chars)\n",
    "\n",
    "train_steps = len(vocab) // train_batch_size\n",
    "val_steps = len(val_tokens) // val_batch_size\n",
    "\n",
    "# Compile the model.\n",
    "model, encoder_model, decoder_model = seq2seq(\n",
    "hidden_size, nb_input_chars, nb_target_chars)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqcV08mAB1X0"
   },
   "source": [
    "Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "EhQttQ8oBzy0",
    "outputId": "dade4afb-a273-49ad-dbdf-d456a04f637e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-449f2a2ae605>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Main Epoch {:d}/{:d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     train_encoder, train_decoder, train_target = transform(\n\u001b[0;32m      5\u001b[0m         vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    print('Main Epoch {:d}/{:d}'.format(epoch + 1, nb_epochs))\n",
    "\n",
    "    train_encoder, train_decoder, train_target = transform(\n",
    "        vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
    "\n",
    "    train_encoder_batch = batch(train_encoder, maxlen, input_ctable,\n",
    "                                train_batch_size, reverse)\n",
    "    train_decoder_batch = batch(train_decoder, maxlen, target_ctable,\n",
    "                                train_batch_size)\n",
    "    train_target_batch  = batch(train_target, maxlen, target_ctable,\n",
    "                                train_batch_size)    \n",
    "\n",
    "    val_encoder_batch = batch(val_encoder, maxlen, input_ctable,\n",
    "                              val_batch_size, reverse)\n",
    "    val_decoder_batch = batch(val_decoder, maxlen, target_ctable,\n",
    "                              val_batch_size)\n",
    "    val_target_batch  = batch(val_target, maxlen, target_ctable,\n",
    "                              val_batch_size)\n",
    "\n",
    "    train_loader = datagen(train_encoder_batch,\n",
    "                            train_decoder_batch, train_target_batch)\n",
    "    val_loader = datagen(val_encoder_batch,\n",
    "                          val_decoder_batch, val_target_batch)\n",
    "\n",
    "    model.fit_generator(train_loader,\n",
    "                        steps_per_epoch=train_steps,\n",
    "                        epochs=1, verbose=1,\n",
    "                        validation_data=val_loader,\n",
    "                        validation_steps=val_steps)\n",
    "    # On epoch end - decode a batch of misspelled tokens from the\n",
    "    # validation set to visualize speller performance.\n",
    "    nb_tokens = 10\n",
    "    input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
    "        val_encoder, val_target, input_ctable, target_ctable,\n",
    "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
    "        sample_mode=sample_mode, random=True)\n",
    "\n",
    "    print('-')\n",
    "    print('Input tokens:  ', input_tokens)\n",
    "    print('Decoded tokens:', decoded_tokens)\n",
    "    print('Target tokens: ', target_tokens)\n",
    "    print('-')\n",
    "\n",
    "    # Save the model at end of each epoch.\n",
    "    model_file = '_'.join(['seq2seq', 'epoch', str(epoch + 1)]) + '.h5'\n",
    "    save_dir = 'checkpoints'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, model_file)\n",
    "    print('Saving full model to {:s}'.format(save_path))\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0vFPMdKCn_F"
   },
   "source": [
    "# Test the model on a provided sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e0HTdGEB-4e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5cf9b349c7f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtext\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": [
    "error_rate = 0.6\n",
    "model_path = './models/seq2seq.h5'\n",
    "hidden_size = 512\n",
    "\n",
    "test_sentence = input() or 'The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.'\n",
    "\n",
    "text  = read_text(data_path, books)\n",
    "vocab = tokenize(text)\n",
    "vocab = list(filter(None, set(vocab)))\n",
    "# `maxlen` is the length of the longest word in the vocabulary\n",
    "# plus two SOS and EOS characters.\n",
    "maxlen = max([len(token) for token in vocab]) + 2\n",
    "train_encoder, train_decoder, train_target = transform(\n",
    "    vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
    "\n",
    "tokens = tokenize(test_sentence)\n",
    "tokens = list(filter(None, tokens))\n",
    "nb_tokens = len(tokens)\n",
    "misspelled_tokens, _, target_tokens = transform(\n",
    "    tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
    "\n",
    "input_chars = set(' '.join(train_encoder))\n",
    "target_chars = set(' '.join(train_decoder))\n",
    "input_ctable = CharacterTable(input_chars)\n",
    "target_ctable = CharacterTable(target_chars)\n",
    "\n",
    "encoder_model, decoder_model = restore_model(model_path, hidden_size)\n",
    "\n",
    "input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
    "    misspelled_tokens, target_tokens, input_ctable, target_ctable,\n",
    "    maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
    "    sample_mode=sample_mode, random=False)\n",
    "\n",
    "print('-')\n",
    "print('Input sentence:  ', ' '.join([token for token in input_tokens]))\n",
    "print('-')\n",
    "print('Decoded sentence:', ' '.join([token for token in decoded_tokens]))\n",
    "print('-')\n",
    "print('Target sentence: ', ' '.join([token for token in target_tokens]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spell checker NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
